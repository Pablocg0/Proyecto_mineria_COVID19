{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Data Preprocessing and Feature Engineering\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#Model Selection and Validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_json(\"data/tweets_0.json\", orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 36 columns):\n",
      "_id                          9999 non-null object\n",
      "contributors                 0 non-null float64\n",
      "coordinates                  361 non-null object\n",
      "created_at                   9999 non-null datetime64[ns]\n",
      "display_text_range           5573 non-null object\n",
      "entities                     9999 non-null object\n",
      "extended_entities            1325 non-null object\n",
      "extended_tweet               1873 non-null object\n",
      "favorite_count               9999 non-null int64\n",
      "favorited                    9999 non-null bool\n",
      "filter_level                 9999 non-null object\n",
      "geo                          361 non-null object\n",
      "id                           9999 non-null int64\n",
      "id_str                       9999 non-null int64\n",
      "in_reply_to_screen_name      4791 non-null object\n",
      "in_reply_to_status_id        4508 non-null float64\n",
      "in_reply_to_status_id_str    4508 non-null float64\n",
      "in_reply_to_user_id          4791 non-null float64\n",
      "in_reply_to_user_id_str      4791 non-null float64\n",
      "is_quote_status              9999 non-null bool\n",
      "lang                         9999 non-null object\n",
      "place                        9999 non-null object\n",
      "possibly_sensitive           2278 non-null float64\n",
      "quote_count                  9999 non-null int64\n",
      "quoted_status                1626 non-null object\n",
      "quoted_status_id             1626 non-null float64\n",
      "quoted_status_id_str         1626 non-null float64\n",
      "quoted_status_permalink      1626 non-null object\n",
      "reply_count                  9999 non-null int64\n",
      "retweet_count                9999 non-null int64\n",
      "retweeted                    9999 non-null bool\n",
      "source                       9999 non-null object\n",
      "text                         9999 non-null object\n",
      "timestamp_ms                 9999 non-null datetime64[ns]\n",
      "truncated                    9999 non-null bool\n",
      "user                         9999 non-null object\n",
      "dtypes: bool(4), datetime64[ns](2), float64(8), int64(6), object(16)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero convertimos todo el teto a lower case para evitar que cada palabra se cuente doble\n",
    "df['text']=df.text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       @beatrizgmuller @lopezobrador_ @hlgatell @john...\n",
       "1                                             pue' ser...\n",
       "2       @fermg33_ no quiero ser mamona, pero le falta ...\n",
       "3       @webcamsdemexico @pueblaesmexico un cocodrilo ...\n",
       "4               #lopezlargateya #lopezfracasopresidencial\n",
       "5                                  ‚ö†Ô∏èüëáüèª‚ö†Ô∏èüëáüèª‚ö†Ô∏èüëáüèª‚ö†Ô∏èüëáüèª‚ö†Ô∏èüëáüèª‚ö†Ô∏è\n",
       "6            @atalia95 luego esto https://t.co/hlyjcpumjf\n",
       "7                         qu√© pinches ganas de cosa seria\n",
       "8                                me record√≥ a alguien üôÉüôÑü§©\n",
       "9              @manuevas ac√° en mex les decimos cachondos\n",
       "10                                   pero qu√© necesidad üòî\n",
       "11      todos necesitamos ese momento de oscuridad par...\n",
       "12      @hectorr18264358 @vanhot7 ase. es. te aborda q...\n",
       "13      - incendia la casa y culpa a winston churchill...\n",
       "14      @sinlineamx de q se preocupan\\nson perros ladr...\n",
       "15                 @ameliayanez_ jajaja ya tiene rato eso\n",
       "16      ya le quiero contar a nuestros hijos que su ma...\n",
       "17      @luiguiluis3 @javierrn @genarolozano @lopezobr...\n",
       "18      @sacmexcdmx @gobcdmx @claudiashein @c5_cdmx @s...\n",
       "19      @draceratiana @babbyco @koshkaelgatito @anacue...\n",
       "20      @ivan_venegas_e es verdad! por que al final de...\n",
       "21                                   no t tocaba kiki uwu\n",
       "22      @the_bob_geldof 2020, covid-19, live aid for t...\n",
       "23      @carlosantoniomz @erickrdgz7 @ldarioag @lableg...\n",
       "24      @anaerii jaja, te voy a aplicar lo que le dije...\n",
       "25      cuarentena d√≠a 13: \\n\\nya me avent√© todos los ...\n",
       "26      @jorgeblancod @rayados tienes toda la raz√≥n co...\n",
       "27           tres  blanco y negro https://t.co/l8oxwebjh3\n",
       "28                                   bien muy bien,üëçüëçüëçüëèüëèüëè\n",
       "29      osvaldo y yo estamos en netflix party y lo rec...\n",
       "                              ...                        \n",
       "9969    #panther #tattoo en submit tattoo studio https...\n",
       "9970            @jocelyn_karinaa yo quiero pero a carol üòõ\n",
       "9971    @felipecalderon cuando se nace pendejo no hay ...\n",
       "9972                                 @andreao11638669 buu\n",
       "9973    @losinmamables ya solo escuch√≥ esa canci√≥n cad...\n",
       "9974                                     atenci√≥n #xalapa\n",
       "9975    5Ô∏è‚É£ deportistas que marcaron mi vida:\\n\\n‚≠êÔ∏è@an...\n",
       "9976                             @marlonmontana1 el lunes\n",
       "9977                                         @iwicho2 üò†üò†üò†\n",
       "9978              no le ser√° muy f√°cil a este malnacido!!\n",
       "9979    @sviisol @el_universal_mx all√° dejen a esta ba...\n",
       "9980                                           @romom_j .\n",
       "9981                  ac√° en mi pueblito son muy com√∫nes.\n",
       "9982                    yo quiero uno....d√≥nde lo pido ‚ÅâÔ∏è\n",
       "9983    #mames que chido episodio y el intro #mames #g...\n",
       "9984                  @alonsoh47443683 me dejas probarla?\n",
       "9985    #tardes\\n#felizsabado\\n#desestres\\n#buenasvibr...\n",
       "9986    estoy a una rinoplastia de parecer prima de da...\n",
       "9987                                   @alanbonb ya vente\n",
       "9988    @snako_fbj @igdec @charlymilk @hlgatell pobre ...\n",
       "9989    y en la afamada secci√≥n, todo se trata de mi, ...\n",
       "9990    actuemos juntos. qu√©date en casa y frenemos la...\n",
       "9991    #quer√©taro ü¶†| instalan cerco sanitario en el h...\n",
       "9992                          @principitored kerico kiero\n",
       "9993    as√≠ vivi√≥ rosalino el pescado de moctezuma\\n\\n...\n",
       "9994    ay a √©sta mujer le encanta jugar con la inteli...\n",
       "9995                                            @malorama\n",
       "9996    @ruben_esponda @paulinagreenham entonces el ti...\n",
       "9997    la @dmspdgo informa que ya fue localizado el v...\n",
       "9998    @lobito_wallst @fernandodelrio yo creo que ya ...\n",
       "Name: text, Length: 9999, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procederemos a lematizar los tweets\n",
    "salida=[]\n",
    "for i in df.text:\n",
    "    doc=nlp(i)\n",
    "    for token in doc:\n",
    "        salida.append(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20728"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(salida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Palabras frecuentes\n",
    "import collections\n",
    "import operator\n",
    "unicas=collections.Counter(salida)\n",
    "unicas_sort=sorted(unicas.items(), key=operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 3382),\n",
       " (',', 3334),\n",
       " ('lo', 3013),\n",
       " ('que', 3003),\n",
       " ('.', 2764),\n",
       " ('el', 2488),\n",
       " ('#', 2082),\n",
       " ('ser', 2072),\n",
       " ('a', 2027),\n",
       " ('!', 2025),\n",
       " ('no', 2000),\n",
       " ('en', 1949),\n",
       " ('y', 1917),\n",
       " ('‚Ä¶', 1879),\n",
       " ('uno', 1354),\n",
       " ('\\n', 1216),\n",
       " ('?', 1073),\n",
       " ('me', 1042),\n",
       " ('se', 948),\n",
       " ('estar', 865),\n",
       " ('por', 786),\n",
       " ('mi', 740),\n",
       " ('con', 704),\n",
       " ('si', 694),\n",
       " ('este', 692),\n",
       " ('ya', 690),\n",
       " ('...', 632),\n",
       " ('tener', 612),\n",
       " ('hacer', 609),\n",
       " ('parir', 607),\n",
       " ('haber', 560),\n",
       " ('todo', 554),\n",
       " ('los', 502),\n",
       " ('te', 501),\n",
       " ('_', 496),\n",
       " ('ir', 477),\n",
       " (' ', 454),\n",
       " ('su', 450),\n",
       " ('comer', 444),\n",
       " ('ver', 439),\n",
       " ('Ô∏è', 437),\n",
       " ('del', 435),\n",
       " ('\\n\\n', 433),\n",
       " ('pero', 431),\n",
       " ('casar', 425),\n",
       " ('decir', 416),\n",
       " ('al', 400),\n",
       " ('üòÇ', 394),\n",
       " ('ese', 378),\n",
       " ('m√°s', 365),\n",
       " ('yo', 363),\n",
       " ('querer', 346),\n",
       " ('tu', 340),\n",
       " ('@hlgatell', 333),\n",
       " ('\"', 329),\n",
       " ('poder', 324),\n",
       " ('ü§£', 301),\n",
       " (':', 297),\n",
       " ('le', 290),\n",
       " ('¬ø', 278),\n",
       " ('@lopezobrador', 274),\n",
       " ('d√≠a', 255),\n",
       " ('o', 251),\n",
       " ('qu√©', 242),\n",
       " ('salir', 236),\n",
       " ('asir', 224),\n",
       " ('bien', 218),\n",
       " ('üèª', 216),\n",
       " ('mucho', 214),\n",
       " ('dar', 211),\n",
       " ('cuarentena', 198),\n",
       " ('bueno', 189),\n",
       " ('üòç', 189),\n",
       " ('hoy', 189),\n",
       " ('cuando', 187),\n",
       " ('muy', 179),\n",
       " ('üôè', 176),\n",
       " ('porque', 175),\n",
       " ('solo', 173),\n",
       " ('ahora', 169),\n",
       " ('seguir', 169),\n",
       " ('¬°', 160),\n",
       " ('pasar', 160),\n",
       " ('gracia', 159),\n",
       " ('ni', 159),\n",
       " ('saber', 159),\n",
       " ('gente', 152),\n",
       " ('i', 148),\n",
       " ('les', 145),\n",
       " ('creer', 145),\n",
       " ('deber', 145),\n",
       " ('m√©xico', 144),\n",
       " ('sin', 144),\n",
       " ('otro', 138),\n",
       " ('jajaja', 137),\n",
       " ('nadar', 135),\n",
       " ('‚Äú', 135),\n",
       " ('mejor', 133),\n",
       " ('t√∫', 132),\n",
       " ('üò≠', 132),\n",
       " ('vida', 130),\n",
       " ('siempre', 130),\n",
       " ('-', 129),\n",
       " ('(', 127),\n",
       " ('mexico', 127),\n",
       " ('‚ù§', 127),\n",
       " ('\\U0001f97a', 124),\n",
       " ('jajajaja', 121),\n",
       " ('amigo', 119),\n",
       " ('the', 117),\n",
       " ('poner', 117),\n",
       " ('hasta', 117),\n",
       " ('‚Äù', 117),\n",
       " ('dejar', 117),\n",
       " ('quedateencasa', 116),\n",
       " ('pues', 116),\n",
       " ('desde', 114),\n",
       " ('üèª\\u200d', 114),\n",
       " ('üëè', 113),\n",
       " ('personar', 113),\n",
       " ('vez', 112),\n",
       " ('tambi√©n', 112),\n",
       " (')', 112),\n",
       " ('s√≠', 112),\n",
       " ('acabar', 109),\n",
       " ('gustar', 108),\n",
       " ('üèº', 107),\n",
       " ('presidente', 104),\n",
       " ('nuestro', 100),\n",
       " ('quien', 99),\n",
       " ('andar', 97),\n",
       " ('pensar', 97),\n",
       " ('sentir', 94),\n",
       " ('coronavirus', 93),\n",
       " ('tan', 93),\n",
       " ('....', 92),\n",
       " ('unir', 92),\n",
       " ('semana', 90),\n",
       " ('‚ôÄ', 90),\n",
       " ('entender', 90),\n",
       " ('q', 88),\n",
       " ('trabajar', 86),\n",
       " ('e', 86),\n",
       " ('ü§§', 85),\n",
       " ('salud', 85),\n",
       " ('@felipecalderon', 85),\n",
       " ('esperar', 84),\n",
       " ('quedar', 84),\n",
       " ('a√±o', 84),\n",
       " ('venir', 84),\n",
       " ('tiempo', 83),\n",
       " ('foto', 83),\n",
       " ('alguien', 82),\n",
       " ('üëç', 82),\n",
       " ('@omarfayad', 82),\n",
       " ('ü§¶', 82),\n",
       " ('hablar', 82),\n",
       " ('\\U0001f970', 81),\n",
       " ('1', 80),\n",
       " ('nunca', 80),\n",
       " ('tanto', 80),\n",
       " ('llamar', 80),\n",
       " ('algo', 79),\n",
       " ('valer', 79),\n",
       " ('se√±or', 79),\n",
       " ('@ssalud_mx', 79),\n",
       " ('entrar', 79),\n",
       " ('‚ôÇ', 78),\n",
       " ('in', 77),\n",
       " ('coser', 76),\n",
       " ('üôÑ', 76),\n",
       " ('llevar', 76),\n",
       " ('pendejo', 76),\n",
       " ('vivir', 76),\n",
       " ('momento', 75),\n",
       " ('nadie', 75),\n",
       " ('aqu√≠', 75),\n",
       " ('contar', 74),\n",
       " ('tomar', 74),\n",
       " ('hermoso', 74),\n",
       " ('hora', 74),\n",
       " ('üî•', 74),\n",
       " ('c√≥mo', 73),\n",
       " ('madre', 73),\n",
       " ('necesitar', 72),\n",
       " ('amar', 72),\n",
       " ('you', 72),\n",
       " ('pedir', 72),\n",
       " ('üá≤', 71),\n",
       " ('2', 71),\n",
       " ('noche', 71),\n",
       " ('√©l', 70),\n",
       " ('üò±', 69),\n",
       " ('do', 69),\n",
       " ('√∫ltimo', 69),\n",
       " ('ganar', 68),\n",
       " ('volver', 68),\n",
       " ('üáΩ', 68),\n",
       " ('3', 68),\n",
       " ('mismo', 68)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicas_sort[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the init parameter for the k-means should be 'k-means++' or 'random' or an ndarray, 'K-means++' (type '<class 'str'>') was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b6e8a04ed4cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrue_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'K-means++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/armando/.local/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    970\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/armando/.local/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                 random_state=random_state)\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/armando/.local/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;31m# init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n\u001b[0;32m--> 529\u001b[0;31m                               x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initialization complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/armando/.local/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_init_centroids\u001b[0;34m(X, k, init, random_state, x_squared_norms, init_size)\u001b[0m\n\u001b[1;32m    758\u001b[0m         raise ValueError(\"the init parameter for the k-means should \"\n\u001b[1;32m    759\u001b[0m                          \u001b[0;34m\"be 'k-means++' or 'random' or an ndarray, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m                          \"'%s' (type '%s') was passed.\" % (init, type(init)))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: the init parameter for the k-means should be 'k-means++' or 'random' or an ndarray, 'K-means++' (type '<class 'str'>') was passed."
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=[word for word in stopwords.words('spanish')]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stopwords)\n",
    "X=vectorizer.fit_transform(df.text)\n",
    "true_k = 4\n",
    "model= KMeans(n_clusters=true_k, init='K-means++', max_iter=10000, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
